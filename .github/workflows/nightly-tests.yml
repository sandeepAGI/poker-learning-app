name: Nightly Test Suite

on:
  schedule:
    - cron: '0 2 * * *'  # 2 AM UTC daily
  workflow_dispatch:      # Allow manual trigger

permissions:
  issues: write
  contents: read

jobs:
  slow-backend-tests:
    name: Slow Backend Tests
    runs-on: ubuntu-latest
    timeout-minutes: 180

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.12
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'
          cache: 'pip'

      - name: Install dependencies
        run: |
          cd backend
          pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest-asyncio pytest-timeout requests

      - name: Run slow backend tests with crash detection
        run: |
          # Enable core dumps for segfault detection
          ulimit -c unlimited
          echo "Core dump limit: $(ulimit -c)"

          # Run tests with Python fault handler enabled
          # Exclude monthly tests (fuzzing + stress) - those run monthly
          PYTHONPATH=backend python -X faulthandler -m pytest backend/tests/ \
            -v --tb=long \
            -m "slow and not monthly" \
            --timeout=600 \
            --ignore=backend/tests/test_bugs_real.py \
            --ignore=backend/tests/test_multiple_winners.py

          # Check for core dumps (segfaults)
          if compgen -G "core*" > /dev/null; then
            echo "::error::❌ Segmentation fault detected!"
            echo "Core dumps found:"
            ls -lh core* || true
            file core* || true
            echo "This indicates a crash in C extensions (likely treys library)"
            exit 1
          fi

          echo "✅ No segfaults detected"

      - name: Test Summary
        if: always()
        run: |
          echo "## Nightly Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Slow tests executed:**" >> $GITHUB_STEP_SUMMARY
          echo "- Edge cases: 350+ scenarios" >> $GITHUB_STEP_SUMMARY
          echo "- RNG fairness: Statistical validation" >> $GITHUB_STEP_SUMMARY
          echo "- Performance: Benchmarks" >> $GITHUB_STEP_SUMMARY
          echo "- Concurrency: Multi-connection stress" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Not included in nightly:**" >> $GITHUB_STEP_SUMMARY
          echo "- Monthly marathon tests (fuzzing + stress)" >> $GITHUB_STEP_SUMMARY
          echo "  - Run 1st of each month with 3-hour timeout" >> $GITHUB_STEP_SUMMARY
          echo "  - Includes: 200-game AI marathon, action fuzzing" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Note:** E2E tests (21 tests) run in comprehensive workflow on every push" >> $GITHUB_STEP_SUMMARY

      - name: Upload results on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: nightly-test-results
          path: |
            htmlcov/
            pytest-results.xml
          retention-days: 7

      - name: Create issue on failure
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            const runUrl = `https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `Nightly Tests Failed (${new Date().toISOString().split('T')[0]})`,
              body: `## Nightly Test Failure\n\n**Workflow**: Nightly Test Suite\n**Run**: ${runUrl}\n**Date**: ${new Date().toISOString()}\n\n### Action Required\n\n1. Review logs: [View Run](${runUrl})\n2. Investigate failure cause\n3. Create fix or update this issue with findings\n4. Close issue when resolved\n\n### Investigation Checklist\n\n- [ ] Logs reviewed\n- [ ] Root cause identified\n- [ ] Fix implemented or issue documented\n- [ ] Verified fix in next nightly run\n\n### Test Categories\n\n- Stress tests (200-game AI marathon)\n- Performance benchmarks\n- RNG fairness (statistical)\n- Action fuzzing\n- Concurrency tests\n- Edge case scenarios (350+)\n- User scenarios (multi-hand)\n- WebSocket simulation\n\n**Note:** E2E tests (21 tests) run in comprehensive workflow on every push.`,
              labels: ['ci-failure', 'nightly', 'needs-investigation', 'automated']
            });
